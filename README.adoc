= R2DBC-DSL

This module provides `R2dbcRepository` for Web Querying throw REST API and Criteria building via `Dsl` class. Working with PostgreSQL, MySQL, Oracle, MS SQL.

The internal extension of the base repository `R2dbcRepository` in Spring Data R2DBC is due to the use of features by all methods, and not just those related to DSL. Also modified a lot of integration tests with DSL features.

The Maven Central dependency instead of the library `spring-data-r2dbc`:

[source,xml]
----
<dependency>
  <groupId>io.github.sevenparadigms</groupId>
  <artifactId>spring-data-r2dbc-dsl</artifactId>
  <version>4.6.10</version>
</dependency>
----

there included common jar with Dsl model (it is convenient when using Reactive Feign Client):

[source,xml]
----
<dependency>
  <groupId>io.github.sevenparadigms</groupId>
  <artifactId>spring-data-r2dbc-dsl-common</artifactId>
  <version>4.6.10</version>
</dependency>
----

That library produce `R2dbcRepository` interface with Dsl extention:
[source,kotlin]
----
interface R2dbcRepository<T, ID> : ReactiveCrudRepository<T, ID> {
    fun findOne(dsl: Dsl): Mono<T>
    fun findAll(dsl: Dsl): Flux<T>
    fun delete(dsl: Dsl): Mono<Integer>
    fun count(dsl: Dsl): Mono<Long>

    fun listener(): Flux<Notification>
    fun saveBatch(models: Iterable<S>): Flux<Result>

    fun evict(Dsl dsl): R2dbcRepository<T, ID>
    fun evict(ID id): R2dbcRepository<T, ID>
    fun evictAll(): R2dbcRepository<T, ID>
    fun put(T value): R2dbcRepository<T, ID>
    fun putAndGet(T value): Mono<T>
    fun get(Dsl dsl): T
    fun get(ID id): T
}
----

and `Dsl` class:
[source,kotlin]
----
data class Dsl {
    val fields: Array<String>
    val query: String
    val lang: String
    val page: Int
    val size: Int
    val sort: String
}
----

== Features

* First Level Cache based on Caffeine with guided expire access and size for each cache from application.yml properties, can be usage as bean for @Cacheable

* Convert First Level Cache to Second Level Cache by property in application.yml: `spring.r2dbc.dsl.secondCache = true` then subscribe to changes from database

* Support Second Level Cache from any Spring CacheManager implementation and may be both working with subscribe to changes from database

* Full First Level Cache manage from repository methods: evict(Dsl dsl), put(Dsl dsl, T value) and get(Dsl dsl): T

* Dsl support all SQL predicates: `=, >, <, >=, <=, like, in, not in, is true, is false, is null, is not null`

* Ability to choose resulted columns in `findAll(dsl: Dsl)` and `findOne(dsl: Dsl)` methods

* Joins to any tables and build criteria on joined columns, also can select joined columns in result

* Annotation `@Id` is not required in model, because column with name `id` in 99% of all tables has name `id`

* Annotation `@Table` is not required in model, because table name in 99% of all tables equals model class name, the camel name automatic convert to sql underlined

* JsonNode type in model automatically converted to JSONB type of PostgreSQL and back

* Expression type in model automatically converted to TEXT type of PostgreSQL and back

* Repository `listener` method is subscribe to listen of all changes of table from database in realtime over `Flux<Notification>`

* Ranked full text search by field `tsv` as default with RUM index operator in `Dsl.fts` method

* Paging and Sorting full support with ordering in several columns at once

* Utility class `FastMethodInvoker` is a modern reflection access to object properties, in performance comparable to direct access.

* Utility class `JsonUtils` is a powerful utils to flexibility manage any json operations around JsonNode type.

* Utility class `R2dbcUtils` can create repository from url: `R2dbcUtils.getRepository("r2dbc:postgresql://postgres:postgres@localhost/abac_rules", AbacRepository::class)`

* Field with new @Equality annotation on each update the value is comparing with previous value from database and throw exception is not equals

* Field with new @ReadOnly annotation on each update the value getting from previous record if not null and setting to current record without comparing

* Field with name `createdAt` type `ZonedDateTime`, `OffsetDateTime` or `LocalDateTime` on update using @ReadOnly annotation logic

* Field with name `updatedAt` and type `ZonedDateTime`, `OffsetDateTime` or `LocalDateTime` on update using @LastModifiedDate annotation logic

* Field with name `version` do not required @Version annotation if it has type `Long`, `Integer`, `ZonedDateTime`, `OffsetDateTime` or `LocalDateTime` for optimistic locking

* Do not need custom repositories for test, because DSL provides the required flexibility without changing business logic

* ApplicationContext implementation as `Beans` utility class with caching resolved beans and registering new beans as ect or recreate by classname

* Utility class SqlField include constants with a most used sql-names in table fields for no handwriting names

== Concepts

The primary idea is to reduce development time when all kinds of criteria can be formed at the frontend:

`localhost:8080/items?query=shops.type==mega,name~~biggest,price>=100 & fields=id,name & page=0 & size=20 & sort=itemType:asc,createdAt:desc`

then generated SQL:

`select id, name from items join shops on items.shop_id = shops.id where shops.type='mega' and name like '%biggest%' and price >= 100 order by item_type asc, created_at desc limit 20 offset 0`

==== Web query predicates (-> sql):

* "con1,(con2),con3" -> con1 or con2 and con3
* "column^^1 2 3" -> column in (1, 2, 3)
* "column!^1 2 3" -> column not in (1, 2, 3)
* "column==value" -> column = value
* "column!=value" -> column != value
* "column" -> column is true
* "!column" -> column is not true
* "@column" -> column is null
* "!@column" -> column is not null
* "column>>value" -> column > value
* "column>=value" -> column >= value
* "column<<value" -> column < value
* "column<=value" -> column <= value
* "column~~value" -> column like '%value%'
* "column@@value" -> column @@ '%value%'

==== Web query columns:

* column -> used as is
* column.type -> join table if column is not JsonNode type (model must contain columnId variable)
* column.header.title -> `column->'header'->>'title'` if column have JsonNode type

In `fields` property also can be selected joined columns or jsonb path to output result:
for example column `shops.type` and `jtree.header.title` in result is mapped to class fields `type` and `title` (in sql mapper to `column->'header'->>'title'`).

[source,kotlin]
----
Dsl.create()
   .equals("brotherTable.jtree.hobby.name", "Konami")
   .isTrue("isMonicStyle")
   .isNull("sisterTable.age")
   .fields("age", "sisterTable.name", "jtree.hobby.description")
----
where after executing the next fields in the model will be set: age, name, description. The secondary idea is using dsl in tests as more readable than jdbcTemplate.


== First and Second Level Cache supporting

Each R2dbcRepository by default activate Caffeine cache as First Level Cache, and it is alive 500 ms. But First Level Cache can be converted to Second Level Cache with property in application.yml:

[source,yaml]
----
spring.r2dbc.dsl.secondCache: true
----

after turn on it is of the all repositories subscribed to listen database table for any changes and after receive event is evicted repository cache.

If you need registered any CacheManager to using in R2dbcRepository as Second Level Cache, then set this property:

[source,yaml]
----
spring.r2dbc.dsl.cacheManager: true
----

Previous property `secondCache` can be worked both with CacheManager of in-memory database [Hazelcast, Redis].

We can manage for each R2dbcRepository Caffeine First Level Cache (also Caffeine as Second Level Cache) with custom timeouts and max size:

[source,yaml]
----
spring.r2dbc.dsl.cache:
  <model class simple name>.expireAfterAccess: 500
  <model class simple name>.expireAfterWrite: 1000
  <model class simple name>.maximumSize: 10000
----

== Subscribe to async database UPDATE/INSERT events:

Before create universal notifier function:
[source,postgresql]
----
create function notify_sender() returns trigger
    language plpgsql
as
$$
BEGIN
    PERFORM pg_notify(
                    TG_TABLE_NAME,
                    json_build_object(
                            'operation', TG_OP,
                            'record', row_to_json(NEW)
                        )::text
                );
    RETURN NULL;
END;
$$;
----
and set to tables notifier by trigger:
[source,postgresql]
----
create trigger table_notify
    after insert or update
    on table
    for each row
execute procedure notify_sender();
----

and last in source code:
[source,kotlin]
----
dslRepository.listener()
          .onBackpressureLatest()
          .concatMap { notification ->
              val json = notification.parameter.toJsonNode()
              if (json["operation"].asText() == "INSERT") {
                  info("database event: $json")
              }            
          }          
----

== Ranked full text search:

Default language may be setting in: `spring.r2dbc.dsl.fts-lang`

or get if nothing from: `Locale.getCurrent()`

or can dynamically set in Dsl class: `Dsl.create().lang('English')`

In table look field by default name `tsv`: `Dsl.create().fts("web query text")`, but field name can be setting in parameter  `Dsl.create().fts("ts_vector", "web query text")`.
[source,postgresql]
----
CREATE TABLE public.jobject
(
    id         uuid                     DEFAULT uuid_generate_v1mc() NOT NULL,
    jtree      jsonb                    NOT NULL,
    jfolder_id uuid                     NOT NULL REFERENCES jfolder (id),
    created_at timestamp with time zone DEFAULT timezone('utc'::text, CURRENT_TIMESTAMP),
    tsv        tsvector,
    PRIMARY KEY (jfolder_id, id)
) PARTITION BY LIST (jfolder_id);

CREATE INDEX idx_jobject_tsv ON jobject USING rum (tsv rum_tsvector_ops);
----

and in source code:

[source,kotlin]
----
dslRepository.findAll(Dsl.create().fts("cool | pencil").equals("jfolderId", folderId).pageable(0, 20))
----

== Batch insert:

Any type of object can be inserted from List because the operation is massive at high speed:

[source,kotlin]
----
dslRepository.saveBatch(listOf(cool1, cool2, pencil1, pencil2))
----
